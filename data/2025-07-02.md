<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via Continual Pre-training](https://arxiv.org/abs/2507.00477)
*Qi Wang,Yixuan Cao,Yifan Liu,Jiangtao Zhao,Ping Luo*

Main category: cs.IR

TL;DR: 本文提出R&R重写器，通过专业文档持续预训练和监督微调改进基于检索的生成式问答系统在专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 用户查询和文档表述之间的差异导致重写模型在专业领域中由于知识有限而表现不佳。

Method: 提出R&R重写器，该模型通过持续预训练在专业文档上，类似学生复习课本备战开卷考试，并支持监督微调。

Result: 实验表明R&R在多个数据集中提高了专业问答系统的表现，有效缩小了查询和文档的差距，同时保持了良好的一般性能。

Conclusion: R&R重写器改善了基于检索的生成式问答系统在专业领域应用，为特殊领域提供了更有效的解决方案。

Abstract: A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system
enhances a large language model's knowledge by retrieving relevant documents
based on user queries. Discrepancies between user queries and document
phrasings often necessitate query rewriting. However, in specialized domains,
the rewriter model may struggle due to limited domain-specific knowledge. To
resolve this, we propose the R\&R (Read the doc before Rewriting) rewriter,
which involves continual pre-training on professional documents, akin to how
students prepare for open-book exams by reviewing textbooks. Additionally, it
can be combined with supervised fine-tuning for improved results. Experiments
on multiple datasets demonstrate that R\&R excels in professional QA across
multiple domains, effectively bridging the query-document gap, while
maintaining good performance in general scenarios, thus advancing the
application of RAG-based QA systems in specialized fields.

</details>


### [2] [On Mitigating Data Sparsity in Conversational Recommender Systems](https://arxiv.org/abs/2507.00479)
*Sixiao Zhang,Mingrui Liu,Cheng Long,Wei Yuan,Hongxu Chen,Xiangyu Zhao,Hongzhi Yin*

Main category: cs.IR

TL;DR: 本文提出了一种名为DACRS的对话推荐系统模型，通过对话增强、知识引导实体建模和对话实体匹配三个模块来解决推荐系统中的数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRSs）在捕捉用户偏好方面存在两方面的数据稀疏问题：对话空间庞大且语言多样；项目空间表现出长尾和稀疏的分布。

Method: 提出CRS模型DACRS，包含对话增强、知识引导实体建模和对话实体匹配三个模块。对话增强模块使用两阶段增强流程来丰富数据并提高泛化能力；知识引导实体建模模块采用知识图谱（KG）进行实体替换和相似性约束，增强实体嵌入的表达能力；对话实体匹配模块通过对话引导的注意力聚合融合对话嵌入和实体嵌入获得包含用户显性和隐性偏好的用户嵌入。

Result: 在两个公共数据集上的大量实验表明DACRS表现处于顶尖水平。

Conclusion: DACRS模型通过三个创新模块有效地提高了对话推荐系统的性能，展示了在数据稀疏环境下的优秀表现。

Abstract: Conversational recommender systems (CRSs) capture user preference through
textual information in dialogues. However, they suffer from data sparsity on
two fronts: the dialogue space is vast and linguistically diverse, while the
item space exhibits long-tail and sparse distributions. Existing methods
struggle with (1) generalizing to varied dialogue expressions due to
underutilization of rich textual cues, and (2) learning informative item
representations under severe sparsity. To address these problems, we propose a
CRS model named DACRS. It consists of three modules, namely Dialogue
Augmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching.
In the Dialogue Augmentation module, we apply a two-stage augmentation pipeline
to augment the dialogue context to enrich the data and improve
generalizability. In the Knowledge-Guided Entity Modeling, we propose a
knowledge graph (KG) based entity substitution and an entity similarity
constraint to enhance the expressiveness of entity embeddings. In the
Dialogue-Entity Matching module, we fuse the dialogue embedding with the
mentioned entity embeddings through a dialogue-guided attention aggregation to
acquire user embeddings that contain both the explicit and implicit user
preferences. Extensive experiments on two public datasets demonstrate the
state-of-the-art performance of DACRS.

</details>


### [3] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: MassTool是一个基于多任务搜索的框架，旨在提升大型语言模型与外部工具交互的效果，通过改进查询表示和工具检索精确度实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 工具检索是让大型语言模型有效地与外部工具交互的关键组成部分。然而，现有的大多数方法主要集中于优化工具表示，常忽略精确查询理解的重要性。

Method: MassTool采用双塔架构：工具使用检测塔和工具检索塔。工具使用检测塔预测是否需要函数调用，而工具检索塔利用基于查询的中心图卷积网络（QC-GCN）进行高效的查询-工具匹配。同时，它还结合了基于搜索的用户意图建模（SUIM）来处理多样化和分布外的查询，以及自适应知识转移（AdaKT）模块进行有效的多任务学习。

Result: 通过联合优化工具使用检测损失、列表式检索损失和对比正则化损失，MassTool建立了一个鲁棒的、双步的顺序决策流程，用以精确地理解查询。大量实验证明其在提升检索精确度方面的有效性。

Conclusion: MassTool为一个提升大型语言模型与外部工具交互的有效方法提供了创新性解决方案，增强了查询表示和工具检索的精确度。

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


### [4] [\texttt{WebANNS}: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers](https://arxiv.org/abs/2507.00521)
*Mugeng Liu,Siqi Zhong,Qi Yang,Yudong Han,Xuanzhe Liu,Yun Ma*

Main category: cs.IR

TL;DR: WebANNS is a new ANNS engine for web browsers using WebAssembly, lazy loading, and heuristics to improve speed and reduce memory usage significantly over existing solutions.


<details>
  <summary>Details</summary>
Motivation: 由于现有的近似最近邻搜索(ANNS)的解决方案无法全面应对浏览器中计算限制、外部存储访问问题和内存利用率等挑战，作者提出了一种为网页浏览器专门设计的ANNS引擎WebANNS。

Method: WebANNS利用WebAssembly克服计算瓶颈，设计了懒加载策略以优化外部存储的数据检索，并采取启发式方法减少内存使用。

Result: 实验表明，与最先进的引擎相比，WebANNS在99%的查询延迟上提高了高达743.8倍的速度提升，同时内存使用减少了高达39%，查询时间从10秒减少到了数十毫秒级别。

Conclusion: WebANNS使得浏览器内的ANNS因用户可接受的低延迟而变得实用。

Abstract: Approximate nearest neighbor search (ANNS) has become vital to modern AI
infrastructure, particularly in retrieval-augmented generation (RAG)
applications. Numerous in-browser ANNS engines have emerged to seamlessly
integrate with popular LLM-based web applications, while addressing privacy
protection and challenges of heterogeneous device deployments. However, web
browsers present unique challenges for ANNS, including computational
limitations, external storage access issues, and memory utilization
constraints, which state-of-the-art (SOTA) solutions fail to address
comprehensively.
  We propose \texttt{WebANNS}, a novel ANNS engine specifically designed for
web browsers. \texttt{WebANNS} leverages WebAssembly to overcome computational
bottlenecks, designs a lazy loading strategy to optimize data retrieval from
external storage, and applies a heuristic approach to reduce memory usage.
Experiments show that \texttt{WebANNS} is fast and memory efficient, achieving
up to $743.8\times$ improvement in 99th percentile query latency over the SOTA
engine, while reducing memory usage by up to 39\%. Note that \texttt{WebANNS}
decreases query time from 10 seconds to the 10-millisecond range in browsers,
making in-browser ANNS practical with user-acceptable latency.

</details>


### [5] [Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support](https://arxiv.org/abs/2507.00535)
*Dietmar Jannach,Amra Delić,Francesco Ricci,Markus Zanker*

Main category: cs.IR

TL;DR: 本文质疑当前小组成员推荐系统假设，提出利用ChatGPT等生成式AI助理工具，通过AI推荐代理帮助小组成员做决策，最终实现更自然的小组决策环境。


<details>
  <summary>Details</summary>
Motivation: 当前关于 گروهی پیشنهاد دهنده سیستم‌ها زیادی در ادبیات موجود است، اما هیچ نمونه‌ای از سیستم‌های گروهی پیشنهاد دهنده در دنیای واقعی یافت نمی‌شود. این امر باعث می‌شود ما در مورد مراکز ارتباطی در گروه‌ها و نحوه تصمیم‌گیری پشتیبانی شده توسط پیشنهادها کنجکاو شویم.

Method: برای بازنگری در این زمینه تحقیقی، نویسندگان پیشنهاد می‌کنند که از توانایی‌های ChatGPT و کمک‌های هوش مصنوعی خلق‌شده استفاده کنند تا سیستم‌های پیشنهاد دهنده گروهی را تغییر دهند.

Result: نتیجه مورد انتظار از این روش این است که سیستم‌های پیشنهاد دهنده گروهی سیستم‌هایی باشند که در آنها اعضای گروه با کمک یک کارگزار پیشنهاد دهنده هوش مصنوعی به صورت خودکار در چت‌ها تعامل می‌کنند و در نهایت به یک محیط تصمیم‌گیری طبیعی‌تر برای گروه‌ها منجر می‌شوند.

Conclusion: نویسندگان برای اتخاذ سیستم‌های پیشنهاد دهنده گروهی در عمل به دعوت قوی‌تری می‌کنند و معتقد هستند که استفاده از هوش مصنوعی می‌تواند به این امر کمک کند.

Abstract: More than twenty-five years ago, first ideas were developed on how to design
a system that can provide recommendations to groups of users instead of
individual users. Since then, a rich variety of algorithmic proposals were
published, e.g., on how to acquire individual preferences, how to aggregate
them, and how to generate recommendations for groups of users. However, despite
the rich literature on the topic, barely any examples of real-world group
recommender systems can be found. This lets us question common assumptions in
academic research, in particular regarding communication processes in a group
and how recommendation-supported decisions are made. In this essay, we argue
that these common assumptions and corresponding system designs often may not
match the needs or expectations of users. We thus call for a reorientation in
this research area, leveraging the capabilities of modern Generative AI
assistants like ChatGPT. Specifically, as one promising future direction, we
envision group recommender systems to be systems where human group members
interact in a chat and an AI-based group recommendation agent assists the
decision-making process in an agentic way. Ultimately, this shall lead to a
more natural group decision-making environment and finally to wider adoption of
group recommendation systems in practice.

</details>


### [6] [Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications](https://arxiv.org/abs/2507.00543)
*Leila Tavakoli,Hamed Zamani*

Main category: cs.IR

TL;DR: 本研究评估了大型语言模型在复杂标注任务中的表现，发现其虽然强但在主观或细粒度任务上难以达到人类水平。通过提出一个人工辅助流程，研究展示了该流程能提升标注可靠性并减少人力投入。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动注解领域逐渐受到关注，本研究旨在探索其在一个具体任务——搜索澄清任务——中的表现，试图解决LLMs在复杂多维标注任务中的局限性。

Method: 研究使用了包含五个细粒度标注子任务的高质量多维度数据集，对LLMs进行了系统测评，并通过受人监督的（HITL）工作流程来优化LLMs的标注效果。

Result: 研究发现LLM的预测结果常常不一致、校准度差且对提示变异敏感。通过采用人工辅助流程，标注的可靠性得到了提升，并减少了高达45%的人力投入。

Conclusion: 尽管LLMs在一般环境下表现出色，但对于主观或细粒度的评估任务，它们仍难以复制人类的表现。本研究的HITL流程为将LLMs在实际评估环境中部署提供了一种相对可扩展、成本效益高且精准的方法。

Abstract: Despite growing interest in using large language models (LLMs) to automate
annotation, their effectiveness in complex, nuanced, and multi-dimensional
labelling tasks remains relatively underexplored. This study focuses on
annotation for the search clarification task, leveraging a high-quality,
multi-dimensional dataset that includes five distinct fine-grained annotation
subtasks. Although LLMs have shown impressive capabilities in general settings,
our study reveals that even state-of-the-art models struggle to replicate
human-level performance in subjective or fine-grained evaluation tasks. Through
a systematic assessment, we demonstrate that LLM predictions are often
inconsistent, poorly calibrated, and highly sensitive to prompt variations. To
address these limitations, we propose a simple yet effective human-in-the-loop
(HITL) workflow that uses confidence thresholds and inter-model disagreement to
selectively involve human review. Our findings show that this lightweight
intervention significantly improves annotation reliability while reducing human
effort by up to 45%, offering a relatively scalable and cost-effective yet
accurate path forward for deploying LLMs in real-world evaluation settings.

</details>


### [7] [EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens](https://arxiv.org/abs/2507.00715)
*Chaoqun Yang,Xinyu Lin,Wenjie Wang,Yongqi Li,Teng Sun,Xianjing Han,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 针对大型语言模型推荐存在的推理延迟问题，文章提出了一个高效的推理框架EARN,利用先前层的信息压缩和集中处理边界token,实现加速和降低内存压力。


<details>
  <summary>Details</summary>
Motivation: 现有模型因巨大的计算开销和KV缓存内存压力而存在高推理延迟,压缩方法效果有限。

Method: 分析注意力模式后,提出框架EARN,通过压缩信息到输入序列的边界token,并只关注这些token来加速。

Result: 三个数据集、两种LLM推荐方法和两种LLM架构的实验表明,加速高达3.79倍,缓存减少80.8%,同时保持准确率。

Conclusion: EARN弥补了效率与效果之间的差距,为LLM推荐的实际部署提供了优势。

Abstract: Large Language Model-based generative recommendation (LLMRec) has achieved
notable success, but it suffers from high inference latency due to massive
computational overhead and memory pressure of KV Cache. Existing KV Cache
reduction methods face critical limitations: cache compression offers marginal
acceleration given recommendation tasks' short decoding steps, while prompt
compression risks discarding vital interaction history. Through systematic
analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)
layer-wise attention sparsity inversion where early layers retain dense
informative patterns while later layers exhibit high redundancy, and 2) dual
attention sinks phenomenon where attention scores concentrate on both head and
tail tokens of input sequences. Motivated by these insights, we propose EARN,
an efficient inference framework that leverages the early layers to compress
information into register tokens placed at the input sequence boundaries, then
focuses solely on these tokens in the subsequent layers. Extensive experiments
on three datasets, two LLMRec methods and two LLM architectures demonstrate
EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction
with better accuracy than the general finetuning approach. Our work bridges the
efficiency-effectiveness gap in LLMRec, offering practical deployment
advantages for industrial scenarios.

</details>


### [8] [WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks](https://arxiv.org/abs/2507.00938)
*Zihao Sun,Meng Fang,Ling Chen*

Main category: cs.IR

TL;DR: 本文提出了WebArXiv，一个基于静态和不随时间改变的arXiv网络任务的基准，用于评估自主网络代理，并发现了一个常见错误模式：刚性的历史回溯。作者提出了一个简单的动态反射机制来解决问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有的基准测试不稳定和过于简化，评估自主网页代理仍然具有挑战性。所以提出一个静态且时间不变的基准WebArXiv，该基准包含基于arXiv平台的网页任务。

Method: 通过静态的Web快照和确定的真实情况及标准化的动作轨迹，WebArXiv确保了可重复和可靠的评估。此外，提出了一个重量级的动态反射机制来使代理在决策过程中可以选怪性获取相关过去步骤。

Result: 通过WebArXiv对十个最先进的网络代理进行评估，结果显示不同代理之间有明显的性能差异，并验证了所提出的反射策略的有效性。

Conclusion: WebArXiv为一个评估自主网络代理提供了稳定的基准，并揭示了网络代理存在的问题并提出了改进策略，这对提高自主网络代理的性能非常有帮助。

Abstract: Recent progress in large language models (LLMs) has enabled the development
of autonomous web agents capable of navigating and interacting with real
websites. However, evaluating such agents remains challenging due to the
instability and inconsistency of existing benchmarks, which often rely on
dynamic content or oversimplified simulations. In this work, we introduce
WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks
grounded in the arXiv platform. WebArXiv ensures reproducible and reliable
evaluation by anchoring tasks in fixed web snapshots with deterministic ground
truths and standardized action trajectories. Through behavioral analysis, we
identify a common failure mode, Rigid History Reflection, where agents
over-rely on fixed interaction histories. To address this, we propose a
lightweight dynamic reflection mechanism that allows agents to selectively
retrieve relevant past steps during decision-making. We evaluate ten
state-of-the-art web agents on WebArXiv. Results demonstrate clear performance
differences across agents and validate the effectiveness of our proposed
reflection strategy.

</details>
