{"id": "2506.20817", "categories": ["cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.20817", "abs": "https://arxiv.org/abs/2506.20817", "authors": ["Ali Tourani", "Fatemeh Nazary", "Yashar Deldjoo"], "title": "RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation", "comment": "20 pages, 6 figures, 5 tables", "summary": "This paper addresses the challenge of developing multimodal recommender\nsystems for the movie domain, where limited metadata (e.g., title, genre) often\nhinders the generation of robust recommendations. We introduce a resource that\ncombines LLM-generated plot descriptions with trailer-derived visual embeddings\nin a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and\ncollaborative filtering. Central to our approach is a data augmentation step\nthat transforms sparse metadata into richer textual signals, alongside fusion\nstrategies (e.g., PCA, CCA) that integrate visual cues. Experimental\nevaluations demonstrate that CCA-based fusion significantly boosts recall\ncompared to unimodal baselines, while an LLM-driven re-ranking step further\nimproves NDCG, particularly in scenarios with limited textual data. By\nreleasing this framework, we invite further exploration of multi-modal\nrecommendation techniques tailored to cold-start, novelty-focused, and\ndomain-specific settings. All code, data, and detailed documentation are\npublicly available at: https://github.com/RecSys-lab/RAG-VisualRec", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7535\u5f71\u9886\u57df\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5267\u60c5\u63cf\u8ff0\u548c\u9884\u544a\u7247\u6d3e\u751f\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7535\u5f71\u9886\u57df\u7684\u63a8\u8350\u7cfb\u7edf\u5e38\u53d7\u9650\u4e8e\u6709\u9650\u7684\u5143\u6570\u636e\uff08\u5982\u6807\u9898\u3001\u79cd\u7c7b\u7b49\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5267\u60c5\u63cf\u8ff0\u4e0e\u9884\u544a\u7247\u89c6\u89c9\u5d4c\u5165\u7edf\u4e00\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u534f\u540c\u8fc7\u6ee4\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6b65\u9aa4\u5c06\u7a00\u758f\u5143\u6570\u636e\u8f6c\u5316\u4e3a\u66f4\u4e30\u5bcc\u7684\u6587\u672c\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u57fa\u4e8eCCA\u7684\u878d\u5408\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u53ec\u56de\u7387\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u91cd\u65b0\u6392\u5e8f\u6b65\u9aa4\u80fd\u663e\u8457\u63d0\u5347NDCG\uff0c\u5c24\u5176\u5728\u6587\u672c\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u6587\u7ae0\u901a\u8fc7\u516c\u5f00\u4ee3\u7801\u3001\u6570\u636e\u548c\u8be6\u7ec6\u6587\u6863\uff0c\u4e3a\u9488\u5bf9\u51b7\u542f\u52a8\u3001\u65b0\u5947\u91cd\u70b9\u548c\u7279\u5b9a\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u8350\u6280\u672f\u63d0\u4f9b\u4e86\u63a2\u7d22\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u73b0\u6709\u7535\u5f71\u9886\u57df\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2506.20844", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2506.20844", "abs": "https://arxiv.org/abs/2506.20844", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "title": "The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers", "comment": "Accepted for ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR'25)", "summary": "Scientific fact-checking aims to determine the veracity of scientific claims\nby retrieving and analysing evidence from research literature. The problem is\ninherently more complex than general fact-checking since it must accommodate\nthe evolving nature of scientific knowledge, the structural complexity of\nacademic literature and the challenges posed by long-form, multimodal\nscientific expression. However, existing approaches focus on simplified\nversions of the problem based on small-scale datasets consisting of abstracts\nrather than full papers, thereby avoiding the distinct challenges associated\nwith processing complete documents. This paper examines the limitations of\ncurrent scientific fact-checking systems and reveals the many potential\nfeatures and resources that could be exploited to advance their performance. It\nidentifies key research challenges within evidence retrieval, including (1)\nevidence-driven retrieval that addresses semantic limitations and topic\nimbalance (2) time-aware evidence retrieval with citation tracking to mitigate\noutdated information, (3) structured document parsing to leverage long-range\ncontext, (4) handling complex scientific expressions, including tables,\nfigures, and domain-specific terminology and (5) assessing the credibility of\nscientific literature. Preliminary experiments were conducted to substantiate\nthese challenges and identify potential solutions. This perspective paper aims\nto advance scientific fact-checking with a specialised IR system tailored for\nreal-world applications.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u73b0\u6709\u7684\u79d1\u5b66\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u6027\u80fd\u7684\u53ef\u80fd\u6027\uff0c\u5305\u62ec\u8bc1\u636e\u68c0\u7d22\u3001\u65f6\u6548\u4fe1\u606f\u8ddf\u8e2a\u3001\u6587\u6863\u89e3\u6790\u3001\u590d\u6742\u79d1\u5b66\u8868\u8fbe\u4ee5\u53ca\u4fe1\u8a89\u8bc4\u4f30\u7b49\u5173\u952e\u6311\u6218\u3002", "motivation": "\u79d1\u5b66\u4e8b\u5b9e\u6838\u67e5\u5177\u6709\u590d\u6742\u6027\uff0c\u56e0\u4e3a\u9700\u8981\u9002\u5e94\u79d1\u5b66\u77e5\u8bc6\u7684\u53d1\u5c55\u3001\u5b66\u672f\u6587\u732e\u7684\u7ed3\u6784\u590d\u6742\u5ea6\u548c\u957f\u5f62\u5f0f\u7684\u591a\u6a21\u6001\u79d1\u5b66\u8868\u8fbe\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5c0f\u578b\u6570\u636e\u96c6\uff0c\u800c\u975e\u5b8c\u6574\u8bba\u6587\uff0c\u4ece\u800c\u56de\u907f\u4e86\u5904\u7406\u5b8c\u6574\u6587\u6863\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u73b0\u6709\u79d1\u5b66\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u5c40\u9650\uff0c\u8bc6\u522b\u4e86\u8bc1\u636e\u68c0\u7d22\u3001\u65f6\u6548\u6027\u8ddf\u8e2a\u3001\u6587\u6863\u89e3\u6790\u3001\u590d\u6742\u8868\u8fbe\u5904\u7406\u548c\u4fe1\u8a89\u8bc4\u4f30\u7b49\u65b9\u9762\u7684\u91cd\u8981\u7814\u7a76\u6311\u6218\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u627e\u5230\u4e86\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e3a\u5b9e\u9645\u5e94\u7528\u91cf\u8eab\u5b9a\u5236\u7684\u4e13\u4e1a\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u63a8\u8fdb\u79d1\u5b66\u4e8b\u5b9e\u6838\u67e5\u3002"}}
{"id": "2506.20854", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.20854", "abs": "https://arxiv.org/abs/2506.20854", "authors": ["Shashank Gupta", "Yiming Liao", "Maarten de Rijke"], "title": "Towards Two-Stage Counterfactual Learning to Rank", "comment": "Accepted at ICTIR 2025 (co-located with SIGIR 2025)", "summary": "Counterfactual learning to rank (CLTR) aims to learn a ranking policy from\nuser interactions while correcting for the inherent biases in interaction data,\nsuch as position bias. Existing CLTR methods assume a single ranking policy\nthat selects top-K ranking from the entire document candidate set. In\nreal-world applications, the candidate document set is on the order of\nmillions, making a single-stage ranking policy impractical. In order to scale\nto millions of documents, real-world ranking systems are designed in a\ntwo-stage fashion, with a candidate generator followed by a ranker. The\nexisting CLTR method for a two-stage offline ranking system only considers the\ntop-1 ranking set-up and only focuses on training the candidate generator, with\nthe ranker fixed. A CLTR method for training both the ranker and candidate\ngenerator jointly is missing from the existing literature. In this paper, we\npropose a two-stage CLTR estimator that considers the interaction between the\ntwo stages and estimates the joint value of the two policies offline. In\naddition, we propose a novel joint optimization method to train the candidate\nand ranker policies, respectively. To the best of our knowledge, we are the\nfirst to propose a CLTR estimator and learning method for two-stage ranking.\nExperimental results on a semi-synthetic benchmark demonstrate the\neffectiveness of the proposed joint CLTR method over baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e24\u9636\u6bb5\u6392\u5e8f\u7cfb\u7edf\u7684\u53cd\u4e8b\u5b9e\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u540c\u65f6\u8bad\u7ec3\u5019\u9009\u751f\u6210\u5668\u548c\u6392\u5e8f\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684CLTR\u65b9\u6cd5\u5047\u8bbe\u5355\u4e00\u7684\u6392\u5e8f\u7b56\u7565\uff0c\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6587\u6863\u7684\u5b9e\u9645\u5e94\u7528\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e24\u9636\u6bb5\u6392\u5e8f\u7cfb\u7edf\u5019\u9009\u96c6\u5e9e\u5927\uff0c\u9700\u8981\u5206\u522b\u8bad\u7ec3\u5019\u9009\u751f\u6210\u5668\u548c\u6392\u5e8f\u5668\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5CLTR\u4f30\u8ba1\u5668\uff0c\u8003\u8651\u4e86\u4e24\u4e2a\u9636\u6bb5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u79bb\u7ebf\u65b9\u5f0f\u4f30\u8ba1\u4e24\u7ea7\u7b56\u7565\u7684\u8054\u5408\u503c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5206\u522b\u8bad\u7ec3\u5019\u9009\u548c\u6392\u5e8f\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5728\u534a\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u63d0\u8bae\u7684\u8054\u5408CLTR\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u7387\u3002", "conclusion": "\u9996\u6b21\u63d0\u51fa\u9488\u5bf9\u4e24\u9636\u6bb5\u6392\u5e8f\u7cfb\u7edf\u7684CLTR\u4f30\u8ba1\u5668\u548c\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6587\u6863\u65f6\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2506.20963", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20963", "abs": "https://arxiv.org/abs/2506.20963", "authors": ["Fangyuan Zhang", "Zhengjun Huang", "Yingli Zhou", "Qintian Guo", "Zhixun Li", "Wensheng Luo", "Di Jiang", "Yixiang Fang", "Xiaofang Zhou"], "title": "EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora", "comment": "Under review", "summary": "Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.", "AI": {"tldr": "EraRAG\u662f\u4e00\u4e2a\u652f\u6301\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u52a8\u6001\u66f4\u65b0\u7684\u65b0\u578b\u591a\u5c42Graph-RAG\u6846\u67b6\uff0c\u91c7\u7528\u8d85\u5e73\u9762\u57fa\u4e8e\u7684\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u5c06\u539f\u59cb\u8bed\u6599\u5e93\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316\u56fe\u7ed3\u6784\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u7684\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5047\u8bbe\u7684\u9759\u6001\u8bed\u6599\u5e93\u9700\u8981\u5728\u65b0\u6587\u6863\u5230\u8fbe\u65f6\u8fdb\u884c\u5168\u9762\u56fe\u91cd\u6784\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "EraRAG\u5229\u7528\u57fa\u4e8e\u8d85\u5e73\u9762\u7684\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u5c06\u539f\u59cb\u8bed\u6599\u5e93\u5206\u533a\u5e76\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316\u56fe\u7ed3\u6784\uff0c\u5141\u8bb8\u9ad8\u6548\u548c\u5c40\u90e8\u7684\u63d2\u5165\u65b0\u6570\u636e\u800c\u4e0d\u7834\u574f\u73b0\u6709\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5728\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cEraRAG\u76f8\u6bd4\u4e8e\u73b0\u6709\u7684Graph-RAG\u7cfb\u7edf\uff0c\u66f4\u65b0\u65f6\u95f4\u548ctoken\u6d88\u8017\u964d\u4f4e\u4e86\u9ad8\u8fbe\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u63d0\u4f9b\u66f4\u4f18\u7684\u51c6\u786e\u6027\u80fd\u3002", "conclusion": "EraRAG\u4e3a\u5fc5\u987b\u64cd\u4f5c\u5728\u6301\u7eed\u589e\u957f\u8bed\u6599\u5e93\u4e0a\u7684RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9053\u8def\uff0c\u517c\u987e\u4e86\u68c0\u7d22\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u4ee5\u5728 https://github.com/EverM0re/EraRAG-Official \u83b7\u53d6\u3002"}}
{"id": "2506.20978", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2506.20978", "abs": "https://arxiv.org/abs/2506.20978", "authors": ["Naihe Feng", "Yi Sui", "Shiyi Hou", "Jesse C. Cresswell", "Ga Wu"], "title": "Response Quality Assessment for Retrieval-Augmented Generation via Conditional Conformal Factuality", "comment": "Accepted by SIGIR 2025 short paper, 5 pages, Code is available at\n  https://github.com/n4feng/ResponseQualityAssessment", "summary": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses\non improving overall question-answering accuracy, often overlooking the quality\nof sub-claims within generated responses. Recent methods that attempt to\nimprove RAG trustworthiness, such as through auto-evaluation metrics, lack\nprobabilistic guarantees or require ground truth answers. To address these\nlimitations, we propose Conformal-RAG, a novel framework inspired by recent\napplications of conformal prediction (CP) on large language models (LLMs).\nConformal-RAG leverages CP and internal information from the RAG mechanism to\noffer statistical guarantees on response quality. It ensures group-conditional\ncoverage spanning multiple sub-domains without requiring manual labelling of\nconformal sets, making it suitable for complex RAG applications. Compared to\nexisting RAG auto-evaluation methods, Conformal-RAG offers statistical\nguarantees on the quality of refined sub-claims, ensuring response reliability\nwithout the need for ground truth answers. Additionally, our experiments\ndemonstrate that by leveraging information from the RAG system, Conformal-RAG\nretains up to 60\\% more high-quality sub-claims from the response compared to\ndirect applications of CP to LLMs, while maintaining the same reliability\nguarantee.", "AI": {"tldr": "Conformal-RAG \u662f\u4e00\u79cd\u57fa\u4e8e\u79ef\u5206\u9884\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u6765\u63d0\u9ad8\u751f\u6210\u56de\u7b54\u7684\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u65e0\u9700\u771f\u5b9e\u7b54\u6848\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0a\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u95ee\u7b54\u7684\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u751f\u6210\u56de\u7b54\u4e2d\u5b50\u58f0\u660e\uff08sub-claims\uff09\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5c1d\u8bd5\u63d0\u9ad8RAG\u53ef\u4fe1\u5ea6\u7684\u65b9\u6cd5\uff0c\u5982\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u7f3a\u4e4f\u6982\u7387\u4fdd\u8bc1\u6216\u9700\u8981\u771f\u5b9e\u7b54\u6848\u3002\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "Conformal-RAG \u5229\u7528\u79ef\u5206\u9884\u6d4b\uff08CP\uff09\u548cRAG\u673a\u5236\u5185\u90e8\u4fe1\u606f\u6765\u63d0\u4f9b\u56de\u7b54\u8d28\u91cf\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002\u5b83\u786e\u4fdd\u8de8\u8d8a\u591a\u4e2a\u5b50\u9886\u57df\u7684\u7ec4\u6761\u4ef6\u8986\u76d6\u7387\uff0c\u800c\u4e0d\u9700\u8981\u4eba\u5de5\u6807\u8bb0\u79ef\u5206\u96c6\u3002", "result": "\u4e0e\u73b0\u6709\u7684RAG\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u76f8\u6bd4\uff0cConformal-RAG\u4e3a\u7ec6\u5316\u5b50\u58f0\u660e\u7684\u8d28\u91cf\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u786e\u4fdd\u56de\u7b54\u53ef\u9760\u6027\u800c\u65e0\u9700\u771f\u5b9e\u7b54\u6848\u3002\u5b9e\u9a8c\u8fd8\u8868\u660e\uff0c\u901a\u8fc7\u5229\u7528RAG\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\uff0cConformal-RAG\u4e0e\u76f4\u63a5\u5c06CP\u5e94\u7528\u4e8eLLM\u76f8\u6bd4\uff0c\u4fdd\u7559\u4e86\u9ad8\u8fbe60%\u7684\u9ad8\u8d28\u91cf\u5b50\u58f0\u660e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u53ef\u9760\u6027\u4fdd\u8bc1\u3002", "conclusion": "Conformal-RAG\u6846\u67b6\u5728\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u4e2d\u751f\u6210\u56de\u7b54\u7684\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u5c55\u793a\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u771f\u5b9e\u7b54\u6848\uff0c\u6709\u6f5c\u529b\u9002\u7528\u4e8e\u590d\u6742\u7684RAG\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.21032", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21032", "abs": "https://arxiv.org/abs/2506.21032", "authors": ["Shuo Yang", "Jiangxia Cao", "Haipeng Li", "Yuqi Mao", "Shuchao Pang"], "title": "RecCoT: Enhancing Recommendation via Chain-of-Thought", "comment": "Work in progress", "summary": "In real-world applications, users always interact with items in multiple\naspects, such as through implicit binary feedback (e.g., clicks, dislikes, long\nviews) and explicit feedback (e.g., comments, reviews). Modern recommendation\nsystems (RecSys) learn user-item collaborative signals from these implicit\nfeedback signals as a large-scale binary data-streaming, subsequently\nrecommending other highly similar items based on users' personalized historical\ninteractions. However, from this collaborative-connection perspective, the\nRecSys does not focus on the actual content of the items themselves but instead\nprioritizes higher-probability signals of behavioral co-occurrence among items.\nConsequently, under this binary learning paradigm, the RecSys struggles to\nunderstand why a user likes or dislikes certain items. To alleviate it, some\nworks attempt to utilize the content-based reviews to capture the semantic\nknowledge to enhance recommender models. However, most of these methods focus\non predicting the ratings of reviews, but do not provide a human-understandable\nexplanation.", "AI": {"tldr": "\u672c\u6587\u5f3a\u8c03\u5f53\u524d\u63a8\u8350\u7cfb\u7edf\u5229\u7528\u9690\u5f0f\u4e8c\u5143\u53cd\u9988\u4fe1\u53f7\u65f6\u5ffd\u89c6\u4e86\u7269\u54c1\u7684\u5b9e\u9645\u5185\u5bb9\uff0c\u5e76\u4e14\u96be\u4ee5\u89e3\u91ca\u7528\u6237\u4e3a\u4f55\u559c\u6b22\u6216\u4e0d\u559c\u6b22\u67d0\u4e9b\u7269\u54c1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u548c\u5185\u5bb9\u7406\u89e3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u7ed3\u5408\u663e\u5f0f\u53cd\u9988\u4fe1\u53f7\u5982\u8bc4\u8bba\u6765\u63d0\u9ad8\u63a8\u8350\u7684\u7cbe\u51c6\u6027\u548c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u6587\u7ae0\u6307\u51fa\u9700\u8981\u5229\u7528\u8bc4\u8bba\u5185\u5bb9\u8fdb\u884c\u8bed\u4e49\u5206\u6790\u4ee5\u589e\u5f3a\u63a8\u8350\u6a21\u578b\uff0c\u5e76\u8bd5\u56fe\u901a\u8fc7\u9884\u6d4b\u8bc4\u8bba\u7684\u8bc4\u5206\u6765\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u5c55\u671b\uff0c\u5e0c\u671b\u63a8\u8350\u7cfb\u7edf\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u559c\u597d\u80cc\u540e\u7684\u7406\u7531\uff0c\u5e76\u4f7f\u63a8\u8350\u7cfb\u7edf\u4e0d\u4ec5\u9ad8\u6548\u8fd8\u80fd\u63d0\u4f9b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002", "conclusion": "\u63a8\u8350\u7cfb\u7edf\u5e94\u7ed3\u5408\u9690\u5f0f\u548c\u663e\u5f0f\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u8bc4\u8bba\u5185\u5bb9\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\uff0c\u4ee5\u671f\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u63a8\u8350\u548c\u66f4\u4f73\u7684\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2506.21368", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21368", "abs": "https://arxiv.org/abs/2506.21368", "authors": ["Matteo Tolloso", "Davide Bacciu", "Shahab Mokarizadeh", "Marco Varesi"], "title": "Real-time and personalized product recommendations for large e-commerce platforms", "comment": "This paper has been accepted for publication at the International\n  Conference on Artificial Neural Networks (ICANN) 2025. The final\n  authenticated version will be available for purchase through the publisher's\n  website. The conference proceedings will be published by Springer in the\n  Lecture Notes in Computer Science (LNCS) series", "summary": "We present a methodology to provide real-time and personalized product\nrecommendations for large e-commerce platforms, specifically focusing on\nfashion retail. Our approach aims to achieve accurate and scalable\nrecommendations with minimal response times, ensuring user satisfaction,\nleveraging Graph Neural Networks and parsimonious learning methodologies.\nExtensive experimentation with datasets from one of the largest e-commerce\nplatforms demonstrates the effectiveness of our approach in forecasting\npurchase sequences and handling multi-interaction scenarios, achieving\nefficient personalized recommendations under real-world constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u65f6\u4e2a\u6027\u5316\u4ea7\u54c1\u63a8\u8350\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u65f6\u5c1a\u96f6\u552e\u7535\u5546\u5e73\u53f0\u3002", "motivation": "\u7535\u5546\u5e73\u53f0\u7684\u7528\u6237\u5e0c\u671b\u901a\u8fc7\u5feb\u901f\u4e14\u4e2a\u6027\u5316\u7684\u63a8\u8350\u627e\u5230\u5fc3\u4eea\u7684\u5546\u54c1\uff0c\u800c\u73b0\u6709\u7684\u63a8\u8350\u7cfb\u7edf\u96be\u4ee5\u5e73\u8861\u54cd\u5e94\u901f\u5ea6\u4e0e\u63a8\u8350\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u4ee5\u53ca\u8282\u4fed\u7684\u5b66\u4e60\u65b9\u6cd5\u6765\u6784\u5efa\u63a8\u8350\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u65e2\u5feb\u901f\u54cd\u5e94\u53c8\u4e2a\u6027\u5316\u63a8\u8350\u7684\u76ee\u6807\u3002", "result": "\u5728\u5927\u578b\u7535\u5546\u5e73\u53f0\u7684\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u9884\u6d4b\u7528\u6237\u7684\u8d2d\u4e70\u5e8f\u5217\uff0c\u5e76\u5904\u7406\u591a\u4ea4\u4e92\u573a\u666f\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u5b9e\u65f6\u6027\u548c\u4e2a\u6027\u5316\u9700\u6c42\u7684\u540c\u65f6\uff0c\u80fd\u6709\u6548\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u5bf9\u4e8e\u65f6\u5c1a\u96f6\u552e\u7c7b\u7535\u5546\u5177\u6709\u79ef\u6781\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
