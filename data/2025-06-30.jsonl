{"id": "2506.21579", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21579", "abs": "https://arxiv.org/abs/2506.21579", "authors": ["Yingzhi He", "Xiaohao Liu", "An Zhang", "Yunshan Ma", "Tat-Seng Chua"], "title": "LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation", "comment": "KDD 2025", "summary": "Sequential recommendation aims to predict users' future interactions by\nmodeling collaborative filtering (CF) signals from historical behaviors of\nsimilar users or items. Traditional sequential recommenders predominantly rely\non ID-based embeddings, which capture CF signals through high-order\nco-occurrence patterns. However, these embeddings depend solely on past\ninteractions, lacking transferable knowledge to generalize to unseen domains.\nRecent advances in large language models (LLMs) have motivated text-based\nrecommendation approaches that derive item representations from textual\ndescriptions. While these methods enhance generalization, they fail to encode\nCF signals-i.e., latent item correlations and preference patterns-crucial for\neffective recommendation. We argue that an ideal embedding model should\nseamlessly integrate CF signals with rich semantic representations to improve\nboth in-domain and out-of-domain recommendation performance.\n  To this end, we propose LLM2Rec, a novel embedding model tailored for\nsequential recommendation, integrating the rich semantic understanding of LLMs\nwith CF awareness. Our approach follows a two-stage training framework: (1)\nCollaborative Supervised Fine-tuning, which adapts LLMs to infer item\nrelationships based on historical interactions, and (2) Item-level Embedding\nModeling, which refines these specialized LLMs into structured item embedding\nmodels that encode both semantic and collaborative information. Extensive\nexperiments on real-world datasets demonstrate that LLM2Rec effectively\nimproves recommendation quality across both in-domain and out-of-domain\nsettings. Our findings highlight the potential of leveraging LLMs to build more\nrobust, generalizable embedding models for sequential recommendation. Our codes\nare available at https://github.com/HappyPointer/LLM2Rec."}
{"id": "2506.21581", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21581", "abs": "https://arxiv.org/abs/2506.21581", "authors": ["Sarthak Chaturvedi", "Anurag Acharya", "Rounak Meyur", "Koby Hayashi", "Sai Munikoti", "Sameera Horawalavithana"], "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains", "comment": null, "summary": "Evaluation benchmark characteristics may distort the true benefits of domain\nadaptation in retrieval models. This creates misleading assessments that\ninfluence deployment decisions in specialized domains. We show that two\nbenchmarks with drastically different features such as topic diversity,\nboundary overlap, and semantic complexity can influence the perceived benefits\nof fine-tuning. Using environmental regulatory document retrieval as a case\nstudy, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)\nfrom federal agencies. We evaluate these models across two benchmarks with\ndifferent semantic structures. Our findings reveal that identical domain\nadaptation approaches show very different perceived benefits depending on\nevaluation methodology. On one benchmark, with clearly separated topic\nboundaries, domain adaptation shows small improvements (maximum 0.61% NDCG\ngain). However, on the other benchmark with overlapping semantic structures,\nthe same models demonstrate large improvements (up to 2.22% NDCG gain), a\n3.6-fold difference in the performance benefit. We compare these benchmarks\nthrough topic diversity metrics, finding that the higher-performing benchmark\nshows 11% higher average cosine distances between contexts and 23% lower\nsilhouette scores, directly contributing to the observed performance\ndifference. These results demonstrate that benchmark selection strongly\ndetermines assessments of retrieval system effectiveness in specialized\ndomains. Evaluation frameworks with well-separated topics regularly\nunderestimate domain adaptation benefits, while those with overlapping semantic\nboundaries reveal improvements that better reflect real-world regulatory\ndocument complexity. Our findings have important implications for developing\nand deploying AI systems for interdisciplinary domains that integrate multiple\ntopics."}
{"id": "2506.21593", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.21593", "abs": "https://arxiv.org/abs/2506.21593", "authors": ["Abu Hanif Muhammad Syarubany", "Chang Dong Yoo"], "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications", "comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers", "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems."}
{"id": "2506.21598", "categories": ["cs.IR", "stat.ME", "H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.21598", "abs": "https://arxiv.org/abs/2506.21598", "authors": ["Purak Jain", "Sandeep Appala"], "title": "SERP Interference Network and Its Applications in Search Advertising", "comment": "This is an extended version of our paper published at the AdKDD 2024\n  workshop, co-located with ACM KDD. CEUR-WS proceedings:\n  https://ceur-ws.org/Vol-3837/paper_12_ceur_paper.pdf", "summary": "Search Engine marketing teams in the e-commerce industry manage global search\nengine traffic to their websites with the aim to optimize long-term\nprofitability by delivering the best possible customer experience on Search\nEngine Results Pages (SERPs). In order to do so, they need to run continuous\nand rapid Search Marketing A/B tests to continuously evolve and improve their\nproducts. However, unlike typical e-commerce A/B tests that can randomize based\non customer identification, their tests face the challenge of anonymized users\non search engines. On the other hand, simply randomizing on products violates\nStable Unit Treatment Value Assumption for most treatments of interest. In this\nwork, we propose leveraging censored observational data to construct bipartite\n(Search Query to Product Ad or Text Ad) SERP interference networks. Using a\nnovel weighting function, we create weighted projections to form unipartite\ngraphs which can then be use to create clusters to randomized on. We\ndemonstrate this experimental design's application in evaluating a new bidding\nalgorithm for Paid Search. Additionally, we provide a blueprint of a novel\nsystem architecture utilizing SageMaker which enables polyglot programming to\nimplement each component of the experimental framework."}
{"id": "2506.21599", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21599", "abs": "https://arxiv.org/abs/2506.21599", "authors": ["Peibo Li", "Shuang Ao", "Hao Xue", "Yang Song", "Maarten de Rijke", "Johan Barth√©lemy", "Tomasz Bednarz", "Flora D. Salim"], "title": "Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation", "comment": null, "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance."}
{"id": "2506.21601", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21601", "abs": "https://arxiv.org/abs/2506.21601", "authors": ["Duong Bach"], "title": "Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization", "comment": "9 pages", "summary": "Multi-vector document retrieval systems, such as ColPali, excel in\nfine-grained matching for complex queries but incur significant storage and\ncomputational costs due to their reliance on high-dimensional patch embeddings\nand late-interaction scoring. To address these challenges, we propose\nHPC-ColPali, a Hierarchical Patch Compression framework that enhances the\nefficiency of ColPali while preserving its retrieval accuracy. Our approach\nintegrates three innovative techniques: (1) K-Means quantization, which\ncompresses patch embeddings into 1-byte centroid indices, achieving up to\n32$\\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing\nVision-Language Model attention weights to retain only the top-$p\\%$ most\nsalient patches, reducing late-interaction computation by up to 60\\% with less\nthan 2\\% nDCG@10 loss; and (3) optional binary encoding of centroid indices\ninto $b$-bit strings ($b=\\lceil\\log_2 K\\rceil$), enabling rapid Hamming\ndistance-based similarity search for resource-constrained environments.\nEvaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\\%\nlower query latency under HNSW indexing while maintaining high retrieval\nprecision. When integrated into a Retrieval-Augmented Generation pipeline for\nlegal summarization, it reduces hallucination rates by 30\\% and halves\nend-to-end latency. These advancements establish HPC-ColPali as a scalable and\nefficient solution for multi-vector document retrieval across diverse\napplications. Code is available at https://github.com/DngBack/HPC-ColPali."}
{"id": "2506.21604", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21604", "abs": "https://arxiv.org/abs/2506.21604", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications."}
{"id": "2506.21617", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21617", "abs": "https://arxiv.org/abs/2506.21617", "authors": ["Hiba Bederina", "Jill-J√™nn Vie"], "title": "Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems", "comment": null, "summary": "The challenge of balancing user relevance and content diversity in\nrecommender systems is increasingly critical amid growing concerns about\ncontent homogeneity and reduced user engagement. In this work, we propose a\nnovel framework that leverages a multi-objective, contextual sequential\nsampling strategy. Item selection is guided by Bayesian updates that\ndynamically adjust scores to optimize diversity. The reward formulation\nintegrates multiple diversity metrics-including the log-determinant volume of a\ntuned similarity submatrix and ridge leverage scores-along with a diversity\ngain uncertainty term to address the exploration-exploitation trade-off. Both\nintra- and inter-batch diversity are modeled to promote serendipity and\nminimize redundancy. A dominance-based ranking procedure identifies\nPareto-optimal item sets, enabling adaptive and balanced selections at each\niteration. Experiments on a real-world dataset show that our approach\nsignificantly improves diversity without sacrificing relevance, demonstrating\nits potential to enhance user experience in large-scale recommendation\nsettings."}
{"id": "2506.21624", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21624", "abs": "https://arxiv.org/abs/2506.21624", "authors": ["Bla≈æ ≈†krlj", "Yonatan Karni", "Grega Ga≈°per≈°iƒç", "Bla≈æ Mramor", "Yulia Stolin", "Martin Jakomin", "Jasna Urbanƒçiƒç", "Yuval Dishi", "Natalia Silberstein", "Ophir Friedler", "Assaf Klein"], "title": "DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation", "comment": "AdKDD 25", "summary": "The Deep and Cross architecture (DCNv2) is a robust production baseline and\nis integral to numerous real-life recommender systems. Its inherent efficiency\nand ability to model interactions often result in models that are both simpler\nand highly competitive compared to more computationally demanding alternatives,\nsuch as Deep FFMs. In this work, we introduce three significant algorithmic\nimprovements to the DCNv2 architecture, detailing their formulation and\nbehavior at scale. The enhanced architecture we refer to as DCN^2 is actively\nused in a live recommender system, processing over 0.5 billion predictions per\nsecond across diverse use cases where it out-performed DCNv2, both offline and\nonline (ab tests). These improvements effectively address key limitations\nobserved in the DCNv2, including information loss in Cross layers, implicit\nmanagement of collisions through learnable lookup-level weights, and explicit\nmodeling of pairwise similarities with a custom layer that emulates FFMs'\nbehavior. The superior performance of DCN^2 is also demonstrated on four\npublicly available benchmark data sets."}
{"id": "2506.21638", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21638", "abs": "https://arxiv.org/abs/2506.21638", "authors": ["Tao Feng", "Zhigang Hua", "Zijie Lei", "Yan Xie", "Shuang Yang", "Bo Long", "Jiaxuan You"], "title": "IRanker: Towards Ranking Foundation Model", "comment": null, "summary": "Ranking tasks are ubiquitous, encompassing applications such as\nrecommendation systems, LLM routing, and item re-ranking. We propose to unify\nthese tasks using a single ranking foundation model (FM), as it eliminates the\nneed for designing different models for each specific ranking task. However,\nunlike general supervision tasks in LLMs, ranking tasks do not have clear\nlabels for supervision, posing great challenges to developing a ranking FM. To\novercome these challenges, we propose IRanker, a ranking FM framework with\nreinforcement learning (RL) and iterative decoding. Our insight is to decompose\nthe complex ranking task into an iterative decoding process that eliminates the\nworst candidate from the candidate pool step by step, which significantly\nreduces the output combinatorial space and better utilizes the limited context\nlength during RL training. We meticulously train and comprehensively evaluate\nan IRanker-3B model on nine datasets across three scenarios: recommendation,\nrouting, and passage ranking. The results show that a single IRanker-3B\nachieves state-of-the-art results on several datasets compared to models of\nsimilar size, and even surpasses the performance of larger models on certain\ndatasets. We further demonstrate the effectiveness of our RL design and the\nrobustness of the iterative mechanism across different LLM sizes. Moreover, we\nconducted both in-domain and out-of-domain zero-shot generalization\nexperiments, which showed that IRanker-3B achieved good generalization on\nin-domain ranking tasks compared to the base LLM by at least 5% improvement.\nSurprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the\nbase model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the\nthoughts generated by IRanker-3B during training could further enhance\nzero-shot LLM performance."}
{"id": "2506.21913", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21913", "abs": "https://arxiv.org/abs/2506.21913", "authors": ["Zunran Wang", "Zheng Shenpeng", "Wang Shenglan", "Minghui Zhao", "Zhonghua Li"], "title": "HyReC: Exploring Hybrid-based Retriever for Chinese", "comment": null, "summary": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness."}
{"id": "2506.21931", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2506.21931", "abs": "https://arxiv.org/abs/2506.21931", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization."}
{"id": "2506.21934", "categories": ["cs.IR", "cs.CV", "I.3.3; I.2.11; H.5.2"], "pdf": "https://arxiv.org/pdf/2506.21934", "abs": "https://arxiv.org/abs/2506.21934", "authors": ["Najmeh Forouzandehmehr", "Reza Yousefi Maragheh", "Sriram Kollipara", "Kai Zhao", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design", "comment": null, "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation."}
{"id": "2506.22026", "categories": ["cs.IR", "cs.AI", "I.2; H.3"], "pdf": "https://arxiv.org/pdf/2506.22026", "abs": "https://arxiv.org/abs/2506.22026", "authors": ["Simra Shahid", "Marissa Radensky", "Raymond Fok", "Pao Siangliulue", "Daniel S. Weld", "Tom Hope"], "title": "Literature-Grounded Novelty Assessment of Scientific Ideas", "comment": null, "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation."}
{"id": "2506.22112", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22112", "abs": "https://arxiv.org/abs/2506.22112", "authors": ["Wenzheng Shu", "Yanxiang Zeng", "Yongxiang Tang", "Teng Sha", "Ning Luo", "Yanhua Cheng", "Xialong Liu", "Fan Zhou", "Peng Jiang"], "title": "Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems", "comment": "Accepted in Companion Proceedings of the ACM Web Conference 2025", "summary": "Offline reinforcement learning (RL) has emerged as a prevalent and effective\nmethodology for real-world recommender systems, enabling learning policies from\nhistorical data and capturing user preferences. In offline RL, reward shaping\nencounters significant challenges, with past efforts to incorporate prior\nstrategies for uncertainty to improve world models or penalize underexplored\nstate-action pairs. Despite these efforts, a critical gap remains: the\nsimultaneous balancing of intrinsic biases in world models and the diversity of\npolicy recommendations. To address this limitation, we present an innovative\noffline RL framework termed Reallocated Reward for Recommender Systems (R3S).\nBy integrating inherent model uncertainty to tackle the intrinsic fluctuations\nin reward predictions, we boost diversity for decision-making to align with a\nmore interactive paradigm, incorporating extra penalizers with decay that deter\nactions leading to diminished state variety at both local and global scales.\nThe experimental results demonstrate that R3S improves the accuracy of world\nmodels and efficiently harmonizes the heterogeneous preferences of the users."}
{"id": "2506.22210", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22210", "abs": "https://arxiv.org/abs/2506.22210", "authors": ["Weronika ≈Åajewska", "Ivica Kostric", "Gabriel Iturra-Bocaz", "Mariam Arustashvili", "Krisztian Balog"], "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses", "comment": null, "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality."}
{"id": "2506.22262", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2506.22262", "abs": "https://arxiv.org/abs/2506.22262", "authors": ["Evgeny Dedov"], "title": "JointRank: Rank Large Set with Single Pass", "comment": "ICTIR'25 Accepted", "summary": "Efficiently ranking relevant items from large candidate pools is a\ncornerstone of modern information retrieval systems -- such as web search,\nrecommendation, and retrieval-augmented generation. Listwise rerankers, which\nimprove relevance by jointly considering multiple candidates, are often limited\nin practice: either by model input size constraints, or by degraded quality\nwhen processing large sets. We propose a model-agnostic method for fast\nreranking large sets that exceed a model input limits. The method first\npartitions candidate items into overlapping blocks, each of which is ranked\nindependently in parallel. Implicit pairwise comparisons are then derived from\nthese local rankings. Finally, these comparisons are aggregated to construct a\nglobal ranking using algorithms such as Winrate or PageRank. Experiments on\nTREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the\n57.68 for full-context listwise approach using gpt-4.1-mini as long-context\nmodel, while reducing latency from 21 to 8 seconds.\n  The implementation of the algorithm and the experiments is available in the\nrepository: https://github.com/V3RGANz/jointrank"}
{"id": "2506.22303", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22303", "abs": "https://arxiv.org/abs/2506.22303", "authors": ["Xinghe Cheng", "Zihan Zhang", "Jiapu Wang", "Liangda Fang", "Chaobo He", "Quanlong Guan", "Shirui Pan", "Weiqi Luo"], "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation", "comment": null, "summary": "Learning path recommendation seeks to provide learners with a structured\nsequence of learning items (e.g., knowledge concepts or exercises) to optimize\ntheir learning efficiency. Despite significant efforts in this area, most\nexisting methods primarily rely on prerequisite relationships, which present\ntwo major limitations: 1) Many educational datasets do not explicitly provide\nprerequisite relationships between knowledge concepts, hindering the\napplication of current learning path recommendation methods. 2) Relying solely\non prerequisite relationships as the sole knowledge structure can impede\nlearning progress and negatively impact student outcomes. To address these\nchallenges, we propose a novel approach, Discrimination Learning Enhances\nLearning Path Recommendation (DLELP), which enhances learning path\nrecommendations by incorporating both prerequisite and similarity relationships\nbetween knowledge concepts. Specifically, we introduce a knowledge concept\nstructure graph generation module that adaptively constructs knowledge concept\nstructure graphs for different educational datasets, significantly improving\nthe generalizability of learning path recommendation methods. We then propose a\nDiscrimination Learning-driven Reinforcement Learning (DLRL) framework, which\nmitigates the issue of blocked learning paths, further enhancing the efficacy\nof learning path recommendations. Finally, we conduct extensive experiments on\nthree benchmark datasets, demonstrating that our method not only achieves\nstate-of-the-art performance but also provides interpretable reasoning for the\nrecommended learning paths."}
{"id": "2506.22356", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22356", "abs": "https://arxiv.org/abs/2506.22356", "authors": ["Kevin Duh", "Eugene Yang", "Orion Weller", "Andrew Yates", "Dawn Lawrie"], "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval", "comment": "5 pages, 1 figure", "summary": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for\nresearching the context of the question, filtering the returned results, and\ngenerating the final answer. The retrieval system was a ColBERT bi-encoder\narchitecture, which represents a passage with many dense tokens. Retrieval used\na local, compressed index of the FineWeb10-BT collection created with PLAID-X,\nusing a model fine-tuned for multilingual retrieval. Query generation from\ncontext was done with Qwen2.5-7B-Instruct, while filtering was accomplished\nwith m2-bert-80M-8k-retrieval. Up to nine passages were used as context to\ngenerate an answer using Falcon3-10B. This system placed 5th in the LiveRAG\nautomatic evaluation for correctness with a score of 1.07."}
{"id": "2506.22372", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22372", "abs": "https://arxiv.org/abs/2506.22372", "authors": ["Maryam Mousavian", "Zahra Abbasiantaeb", "Mohammad Aliannejadi", "Fabio Crestani"], "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement", "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)", "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems."}
