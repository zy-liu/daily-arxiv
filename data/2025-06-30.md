<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 20]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation](https://arxiv.org/abs/2506.21579)
*Yingzhi He,Xiaohao Liu,An Zhang,Yunshan Ma,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 这篇论文提出了一个名为LLM2Rec的新型嵌入模型，它结合了大型语言模型（LLMs）的语义理解和协同过滤（CF）信号，旨在提高序列推荐的质量。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型依赖基于ID的嵌入，这些模型通过高阶共现模式捕获协同过滤信号，但它们缺乏跨领域泛化的能力。文本推荐方法虽然增强了泛化，但无法编码协同过滤信号。

Method: LLM2Rec采用两阶段训练框架：第一阶段是协同监督微调，通过历史交互推断物品关系；第二阶段是物品级嵌入建模，将专门的LLMs细化为编码语义和协同信息的结构化物品嵌入模型。

Result: 在真实世界数据集上的实验表明，LLM2Rec在特定领域和跨领域推荐中都有效提升了推荐质量。

Conclusion: 该研究突出了利用LLMs构建更强大、更具泛化能力的序列推荐嵌入模型的潜力。

Abstract: Sequential recommendation aims to predict users' future interactions by
modeling collaborative filtering (CF) signals from historical behaviors of
similar users or items. Traditional sequential recommenders predominantly rely
on ID-based embeddings, which capture CF signals through high-order
co-occurrence patterns. However, these embeddings depend solely on past
interactions, lacking transferable knowledge to generalize to unseen domains.
Recent advances in large language models (LLMs) have motivated text-based
recommendation approaches that derive item representations from textual
descriptions. While these methods enhance generalization, they fail to encode
CF signals-i.e., latent item correlations and preference patterns-crucial for
effective recommendation. We argue that an ideal embedding model should
seamlessly integrate CF signals with rich semantic representations to improve
both in-domain and out-of-domain recommendation performance.
  To this end, we propose LLM2Rec, a novel embedding model tailored for
sequential recommendation, integrating the rich semantic understanding of LLMs
with CF awareness. Our approach follows a two-stage training framework: (1)
Collaborative Supervised Fine-tuning, which adapts LLMs to infer item
relationships based on historical interactions, and (2) Item-level Embedding
Modeling, which refines these specialized LLMs into structured item embedding
models that encode both semantic and collaborative information. Extensive
experiments on real-world datasets demonstrate that LLM2Rec effectively
improves recommendation quality across both in-domain and out-of-domain
settings. Our findings highlight the potential of leveraging LLMs to build more
robust, generalizable embedding models for sequential recommendation. Our codes
are available at https://github.com/HappyPointer/LLM2Rec.

</details>


### [2] [Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains](https://arxiv.org/abs/2506.21581)
*Sarthak Chaturvedi,Anurag Acharya,Rounak Meyur,Koby Hayashi,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.IR

TL;DR: 不同的评估基准可能会歪曲领域自适应在检索模型中的真实效益，影响特定领域部署决策。


<details>
  <summary>Details</summary>
Motivation: 评估基准的特性可能扭曲领域自适应在检索模型中的真实效益，导致误导性评估，从而影响专门领域的部署决策。

Method: 该研究使用环境监管文档检索作为案例研究，对ColBERTv2模型在政府机构的环境影响声明（EIS）上进行微调，并在具有不同语义结构的两个基准上评估模型。

Result: 研究发现，在主题边界清晰的基准上，领域自适应的改进很小（最大0.61% NDCG增益）；而在语义结构重叠的基准上，模型显示出较大改进（高达2.22% NDCG增益），性能提升差异达3.6倍。通过主题多样性指标比较，发现表现更好的基准具有11%更高的平均余弦距离和23%更低的轮廓分数。

Conclusion: 基准选择强烈决定了专门领域检索系统有效性的评估。具有清晰分离主题的评估框架通常会低估领域自适应的效益，而具有重叠语义边界的那些则显示出更符合现实世界监管文档复杂性的改进。

Abstract: Evaluation benchmark characteristics may distort the true benefits of domain
adaptation in retrieval models. This creates misleading assessments that
influence deployment decisions in specialized domains. We show that two
benchmarks with drastically different features such as topic diversity,
boundary overlap, and semantic complexity can influence the perceived benefits
of fine-tuning. Using environmental regulatory document retrieval as a case
study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)
from federal agencies. We evaluate these models across two benchmarks with
different semantic structures. Our findings reveal that identical domain
adaptation approaches show very different perceived benefits depending on
evaluation methodology. On one benchmark, with clearly separated topic
boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG
gain). However, on the other benchmark with overlapping semantic structures,
the same models demonstrate large improvements (up to 2.22% NDCG gain), a
3.6-fold difference in the performance benefit. We compare these benchmarks
through topic diversity metrics, finding that the higher-performing benchmark
shows 11% higher average cosine distances between contexts and 23% lower
silhouette scores, directly contributing to the observed performance
difference. These results demonstrate that benchmark selection strongly
determines assessments of retrieval system effectiveness in specialized
domains. Evaluation frameworks with well-separated topics regularly
underestimate domain adaptation benefits, while those with overlapping semantic
boundaries reveal improvements that better reflect real-world regulatory
document complexity. Our findings have important implications for developing
and deploying AI systems for interdisciplinary domains that integrate multiple
topics.

</details>


### [3] [PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications](https://arxiv.org/abs/2506.21593)
*Abu Hanif Muhammad Syarubany,Chang Dong Yoo*

Main category: cs.IR

TL;DR: PentaRAG is a novel five-layer module for efficient management of large-language model (LLM) deployments, enhancing response time, GPU cost predictability, and overall system efficiency for various query types.


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）流程无法满足企业级LLM部署对实时更新文档集合、亚秒级延迟和可预测的GPU成本的要求，因此需要更高效的解决方案。

Method: PentaRAG通过结合即时缓存、记忆召回模式、自适应会话记忆和常规检索增强层等五个层次，智能地路由查询，并利用LLM自带的权重和LoRA微调技术来应对不同类型的查询需求。

Result: 在TriviaQA领域测试中，结合LoRA微调和记忆召回层后的PentaRAG使答案相似度提升了约8%，事实准确性提高了约16%，并且能在模拟的九次会话中，将平均延迟从几秒降低到小于1秒，并且显著提升了系统吞吐量至约每秒100,000个查询。

Conclusion: PentaRAG证明了一个分层的路由策略能够在生产级的RAG系统中同时实现实时性、速度和效率，为企业级LLM部署提供了有效解决方案。

Abstract: Enterprise deployments of large-language model (LLM) demand continuously
changing document collections with sub-second latency and predictable GPU cost
requirements that classical Retrieval-Augmented Generation (RAG) pipelines only
partially satisfy. We present PentaRAG, a five-layer module that routes each
query through two instant caches (fixed key-value and semantic), a
memory-recall mode that exploits the LLM's own weights, an adaptive session
memory, and a conventional retrieval-augmentation layer. Implemented with
Mistral-8B, Milvus and vLLM, the system can answer most repeated or
semantically similar questions from low-latency caches while retaining full
retrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined
with the memory-recall layer raises answer similarity by approximately 8% and
factual correctness by approximately 16% over the base model. Under a
nine-session runtime simulation, cache warming reduces mean latency from
several seconds to well below one second and shifts traffic toward the fast
paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to
0.248 seconds per query, roughly half that of a naive RAG baseline, and
sustains an aggregate throughput of approximately 100,000 queries per second on
our setup. These results demonstrate that a layered routing strategy can
deliver freshness, speed, and efficiency simultaneously in production-grade RAG
systems.

</details>


### [4] [SERP Interference Network and Its Applications in Search Advertising](https://arxiv.org/abs/2506.21598)
*Purak Jain,Sandeep Appala*

Main category: cs.IR

TL;DR: 该研究提出了一个方法，使用被限制的观察数据来构造两部分的搜索结果页面（SERP）干扰网络，以提高搜索引擎营销团队在电商行业的长期盈利能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决搜索引擎营销团队在匿名用户中执行A/B测试的难题。

Method: 提出利用被限制的观察数据构建双边（搜索查询到产品广告或文本广告）的SERP干扰网络，并使用新的加权函数形成无向图，之后创建聚类进行随机化。

Result: 使用实验设计评估了竞价算法，并通过SageMaker系统架构蓝图实现了一个实验框架。

Conclusion: 该研究成功解决了在匿名用户中进行A/B测试的难题，并提出了一个实际可应用的系统架构，以提高搜索营销的长期盈利能力。

Abstract: Search Engine marketing teams in the e-commerce industry manage global search
engine traffic to their websites with the aim to optimize long-term
profitability by delivering the best possible customer experience on Search
Engine Results Pages (SERPs). In order to do so, they need to run continuous
and rapid Search Marketing A/B tests to continuously evolve and improve their
products. However, unlike typical e-commerce A/B tests that can randomize based
on customer identification, their tests face the challenge of anonymized users
on search engines. On the other hand, simply randomizing on products violates
Stable Unit Treatment Value Assumption for most treatments of interest. In this
work, we propose leveraging censored observational data to construct bipartite
(Search Query to Product Ad or Text Ad) SERP interference networks. Using a
novel weighting function, we create weighted projections to form unipartite
graphs which can then be use to create clusters to randomized on. We
demonstrate this experimental design's application in evaluating a new bidding
algorithm for Paid Search. Additionally, we provide a blueprint of a novel
system architecture utilizing SageMaker which enables polyglot programming to
implement each component of the experimental framework.

</details>


### [5] [Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2506.21599)
*Peibo Li,Shuang Ao,Hao Xue,Yang Song,Maarten de Rijke,Johan Barthélemy,Tomasz Bednarz,Flora D. Salim*

Main category: cs.IR

TL;DR: 提出Refine-POI框架，通过强化学习来提升语言模型在下一兴趣点推荐中的性能。


<details>
  <summary>Details</summary>
Motivation: 动机：现有的基于语言模型的下一兴趣点推荐模型分为提示模型和监督微调（SFT）模型。提示模型输出灵活性高但精度较低，而SFT模型虽然能达到较高性能，但存在根本性的不匹配问题：下一兴趣点推荐数据并不天然适合监督微调。

Method: 方法：为解决这一问题，本文提出了Refine-POI，一个强化微调框架。通过引入推荐驱动的奖励机制，智能体可以通过每个示例仅使用一个真实目标兴趣点来学习生成top-k推荐列表。

Result: 结果：在真实世界数据集上的实验表明，Refine-POI能达到最先进的top-k推荐性能。

Conclusion: 结论：Refine-POI通过强化学习有效地提升了语言模型在下一兴趣点推荐任务中的表现，并取得了当前最优的性能表现。

Abstract: Large language models (LLMs) have been adopted for next point-of-interest
(POI) recommendation tasks. Typical LLM-based recommenders fall into two
categories: prompt-based and supervised fine-tuning (SFT)-based models.
Prompt-based models generally offer greater output flexibility but deliver
lower accuracy, whereas SFT-based models achieve higher performance yet face a
fundamental mismatch: next POI recommendation data does not naturally suit
supervised fine-tuning. In SFT, the model is trained to reproduce the exact
ground truth, but each training example provides only a single target POI, so
there is no ground truth for producing a top-k list.
  To address this, we propose Refine-POI, a reinforcement fine-tuning framework
for next POI recommendation. We introduce recommendation-driven rewards that
enable LLMs to learn to generate top-k recommendation lists using only one
ground-truth POI per example. Experiments on real-world datasets demonstrate
that Refine-POI achieves state-of-the-art top-k recommendation performance.

</details>


### [6] [Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization](https://arxiv.org/abs/2506.21601)
*Duong Bach*

Main category: cs.IR

TL;DR: HPC-ColPali通过层叠图像块压缩技术和注意力引导动态剪枝提升了ColPali检索系统的效率，在减少存储和计算成本的同时，保留了检索准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决多向量文档检索系统如ColPali由于依赖高维块嵌入和后期交互评分而产生的重大存储和计算成本问题。

Method: 提出了一个层次化的图像块压缩框架，该框架集成了三种创新技术：(1) K-Means量化压缩；(2) 注意力引导的动态剪枝；(3) 中心索引的二进制编码。

Result: 在ViDoRe和SEC-Filings数据集上的评估显示，HPC-ColPali在使用HNSW索引时，查询延迟降低了30-50%，同时保留了高检索精度。当集成到法律摘要的检索增强生成管道中，它降低了30%的错误率并使端到端延迟减半。

Conclusion: HPC-ColPali为跨多种应用场景的多向量文档检索提供了一个可扩展且高效的解决方案。

Abstract: Multi-vector document retrieval systems, such as ColPali, excel in
fine-grained matching for complex queries but incur significant storage and
computational costs due to their reliance on high-dimensional patch embeddings
and late-interaction scoring. To address these challenges, we propose
HPC-ColPali, a Hierarchical Patch Compression framework that enhances the
efficiency of ColPali while preserving its retrieval accuracy. Our approach
integrates three innovative techniques: (1) K-Means quantization, which
compresses patch embeddings into 1-byte centroid indices, achieving up to
32$\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing
Vision-Language Model attention weights to retain only the top-$p\%$ most
salient patches, reducing late-interaction computation by up to 60\% with less
than 2\% nDCG@10 loss; and (3) optional binary encoding of centroid indices
into $b$-bit strings ($b=\lceil\log_2 K\rceil$), enabling rapid Hamming
distance-based similarity search for resource-constrained environments.
Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\%
lower query latency under HNSW indexing while maintaining high retrieval
precision. When integrated into a Retrieval-Augmented Generation pipeline for
legal summarization, it reduces hallucination rates by 30\% and halves
end-to-end latency. These advancements establish HPC-ColPali as a scalable and
efficient solution for multi-vector document retrieval across diverse
applications. Code is available at https://github.com/DngBack/HPC-ColPali.

</details>


### [7] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 本文提出了一种系统性的定量基准框架来衡量跨模态输入整合的信任度，优化了文本、图像、标题和OCR在VisualRAG系统中的权重集成，显著提升了企业文档智能的处理能力。


<details>
  <summary>Details</summary>
Motivation: 由于现有的多模态生成AI评价框架无法有效建立可信赖度，阻碍了在需要极高可靠性的企业环境中的应用部署。因此，本文旨在通过系统化的定量基准测试框架提升跨模态输入的集成可靠度。

Method: 提出了一种测量框架，通过定量技术指标和以用户为中心的信任度测量之间的相关性来确定信任度。评估提议的权重配置：30%文本、15%图像、25%标题和30%OCR，对比了纯文本基线。

Result: 评估显示，在文本、图像、标题和OCR输入下，设定的权重配置使得性能比纯文本基线提高了57.3%，同时保持计算效率。此外，通过基础模型全面的比较评估，显示它们在标题生成和OCR提取方面对信任度有不同的影响。

Conclusion: 该框架为量化及增强多模态RAG在关键企业应用中的信任度提供了严谨的介绍，推动了负责任的人工智能部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [8] [Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems](https://arxiv.org/abs/2506.21617)
*Hiba Bederina,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 本文提出一种新的推荐系统框架，该框架通过多目标上下文序列采样策略来平衡用户的相关性和内容多样性。


<details>
  <summary>Details</summary>
Motivation: 随着对内容单一性和用户参与度下降的关注日益增加，平衡推荐系统中用户相关性和内容多样性的挑战变得日益重要。

Method: 本文提出一种利用多目标、情境序列采样策略的框架。通过贝叶斯更新动态调整分数以优化多样性，并将多种多样性度量标准（包括调整后的相似性子矩阵的对数行列式体积和岭杠杆分数）以及多样性增益不确定性项纳入奖励公式，以解决探索-利用权衡。

Result: 模型同时模拟了批内和批间的多样性，以促进偶然发现并最小化冗余。基于支配的排序程序识别Pareto最优项目集，实现每次迭代的自适应和平衡选择。

Conclusion: 在真实数据集上的实验表明，该方法在提高多样性的同时并未牺牲相关性，证明了其在大规模推荐环境中的提升用户体验的潜力。

Abstract: The challenge of balancing user relevance and content diversity in
recommender systems is increasingly critical amid growing concerns about
content homogeneity and reduced user engagement. In this work, we propose a
novel framework that leverages a multi-objective, contextual sequential
sampling strategy. Item selection is guided by Bayesian updates that
dynamically adjust scores to optimize diversity. The reward formulation
integrates multiple diversity metrics-including the log-determinant volume of a
tuned similarity submatrix and ridge leverage scores-along with a diversity
gain uncertainty term to address the exploration-exploitation trade-off. Both
intra- and inter-batch diversity are modeled to promote serendipity and
minimize redundancy. A dominance-based ranking procedure identifies
Pareto-optimal item sets, enabling adaptive and balanced selections at each
iteration. Experiments on a real-world dataset show that our approach
significantly improves diversity without sacrificing relevance, demonstrating
its potential to enhance user experience in large-scale recommendation
settings.

</details>


### [9] [DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation](https://arxiv.org/abs/2506.21624)
*Blaž Škrlj,Yonatan Karni,Grega Gašperšič,Blaž Mramor,Yulia Stolin,Martin Jakomin,Jasna Urbančič,Yuval Dishi,Natalia Silberstein,Ophir Friedler,Assaf Klein*

Main category: cs.IR

TL;DR: 本文为DCNv2推荐系统架构进行了三项重要算法改进，提出DCN^2架构，提高了信息利用率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 提升推荐系统的效率和信息利用，解决DCNv2架构的局限性。

Method: 进行三项算法改进，包括增强交叉层信息利用、优化碰撞处理和增强相似度建模。

Result: DCN^2在离线和在线测试中均优于DCNv2，并应用在实时推荐系统中。

Conclusion: DCN^2提高了模型性能，更适用于实际应用场合。

Abstract: The Deep and Cross architecture (DCNv2) is a robust production baseline and
is integral to numerous real-life recommender systems. Its inherent efficiency
and ability to model interactions often result in models that are both simpler
and highly competitive compared to more computationally demanding alternatives,
such as Deep FFMs. In this work, we introduce three significant algorithmic
improvements to the DCNv2 architecture, detailing their formulation and
behavior at scale. The enhanced architecture we refer to as DCN^2 is actively
used in a live recommender system, processing over 0.5 billion predictions per
second across diverse use cases where it out-performed DCNv2, both offline and
online (ab tests). These improvements effectively address key limitations
observed in the DCNv2, including information loss in Cross layers, implicit
management of collisions through learnable lookup-level weights, and explicit
modeling of pairwise similarities with a custom layer that emulates FFMs'
behavior. The superior performance of DCN^2 is also demonstrated on four
publicly available benchmark data sets.

</details>


### [10] [IRanker: Towards Ranking Foundation Model](https://arxiv.org/abs/2506.21638)
*Tao Feng,Zhigang Hua,Zijie Lei,Yan Xie,Shuang Yang,Bo Long,Jiaxuan You*

Main category: cs.IR

TL;DR: 本文提出了IRanker框架，使用强化学习和迭代解码来统一不同的排名任务，在多个数据集上实现最佳结果。


<details>
  <summary>Details</summary>
Motivation: 排名任务是普遍存在的，涵盖了推荐系统、LLM路由和物品重新排序等多种应用。该任务统一排名基础模型的需求目的是为了不再需要针对每个特定的排名任务设计不同的模型。

Method: 提出了IRanker框架，结合了强化学习和迭代解码。通过将复杂的排名任务分解为逐步从候选库中移除最差候选者的迭代解码过程，显著减少了输出的组合空间并更好地利用了在强化学习训练期间有限的上下文长度。

Result: 在多个数据集上对IRanker-3B模型进行了详尽的训练和全面评估，结果显示单IRanker-3B在几个数据集上达到了最先进的结果，并在某些数据集上甚至超过了更大模型的性能。在零样本泛化实验中，IRanker-3B在领域内排名任务中至少提高了5%，并在领域外通用LLM任务中至少提高了9%。此外，IRanker-3B在训练期间产生的思考可以进一步提高零样本LLM的性能。

Conclusion: IRanker框架证明了结合强化学习和迭代解码对排名任务的有效性，并展示了其跨越不同LLM大小的鲁棒性。即使在领域外任务中，IRanker-3B也超过了基础模型，这表明了IRanker强大的泛化能力和潜力。

Abstract: Ranking tasks are ubiquitous, encompassing applications such as
recommendation systems, LLM routing, and item re-ranking. We propose to unify
these tasks using a single ranking foundation model (FM), as it eliminates the
need for designing different models for each specific ranking task. However,
unlike general supervision tasks in LLMs, ranking tasks do not have clear
labels for supervision, posing great challenges to developing a ranking FM. To
overcome these challenges, we propose IRanker, a ranking FM framework with
reinforcement learning (RL) and iterative decoding. Our insight is to decompose
the complex ranking task into an iterative decoding process that eliminates the
worst candidate from the candidate pool step by step, which significantly
reduces the output combinatorial space and better utilizes the limited context
length during RL training. We meticulously train and comprehensively evaluate
an IRanker-3B model on nine datasets across three scenarios: recommendation,
routing, and passage ranking. The results show that a single IRanker-3B
achieves state-of-the-art results on several datasets compared to models of
similar size, and even surpasses the performance of larger models on certain
datasets. We further demonstrate the effectiveness of our RL design and the
robustness of the iterative mechanism across different LLM sizes. Moreover, we
conducted both in-domain and out-of-domain zero-shot generalization
experiments, which showed that IRanker-3B achieved good generalization on
in-domain ranking tasks compared to the base LLM by at least 5% improvement.
Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the
base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the
thoughts generated by IRanker-3B during training could further enhance
zero-shot LLM performance.

</details>


### [11] [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
*Zunran Wang,Zheng Shenpeng,Wang Shenglan,Minghui Zhao,Zhonghua Li*

Main category: cs.IR

TL;DR: 本文提出了HyReC，一种专为中文混合检索优化的端到端优化方法，通过整合术语语义联合、全局局部感知编码器及标准化模块来提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 尽管混合检索方法在提升性能方面得到了工业界的广泛关注，但它们在中文检索环境中的应用仍很少被探索。

Method: HyReC通过整合术语的语义联合到表示模型中，使用全局-局部感知编码器（GLAE）促进基于词库和密集检索之间的一致语义共享，并通过最小化两者间的干扰，同时采用标准化模块增强检索方法的互相促进。

Result: 通过在C-MTEB检索基准上的评估，证明了HyReC的有效性。

Conclusion: HyReC展示了在中文检索环境中，混合检索方法的潜力，并通过创新的优化策略显著提升了检索性能。

Abstract: Hybrid-based retrieval methods, which unify dense-vector and lexicon-based
retrieval, have garnered considerable attention in the industry due to
performance enhancement. However, despite their promising results, the
application of these hybrid paradigms in Chinese retrieval contexts has
remained largely underexplored. In this paper, we introduce HyReC, an
innovative end-to-end optimization method tailored specifically for
hybrid-based retrieval in Chinese. HyReC enhances performance by integrating
the semantic union of terms into the representation model. Additionally, it
features the Global-Local-Aware Encoder (GLAE) to promote consistent semantic
sharing between lexicon-based and dense retrieval while minimizing the
interference between them. To further refine alignment, we incorporate a
Normalization Module (NM) that fosters mutual benefits between the retrieval
approaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to
demonstrate its effectiveness.

</details>


### [12] [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
*Reza Yousefi Maragheh,Pratheek Vadla,Priyank Gupta,Kai Zhao,Aysenur Inan,Kehui Yao,Jianpeng Xu,Praveen Kanumala,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: ARAG框架通过多智能体协作机制增强推荐系统的个性化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的方法依赖静态检索策略，无法捕捉动态推荐场景中的用户细分偏好。本工作旨在通过多智能体协作机制改进这一局限。

Method: ARAG整合了四种专门处理不同任务的智能体：用户理解智能体、自然语言推理智能体、情景总结智能体和项目排序智能体，以改善用户长期及会话行为理解。

Result: 在三个数据集上的实验结果显示，ARAG在NDCG@5和Hit@5指标上分别提升了42.1%和35.5%，显著优于标准RAG和最新策略基线。

Conclusion: ARAG证明了将智能体推理集成到检索增强推荐中的有效性，并为基于LLM的个性化推荐提供了新方向。

Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing
recommendation systems by incorporating external context into large language
model prompts. However, existing RAG-based approaches often rely on static
retrieval heuristics and fail to capture nuanced user preferences in dynamic
recommendation scenarios. In this work, we introduce ARAG, an Agentic
Retrieval-Augmented Generation framework for Personalized Recommendation, which
integrates a multi-agent collaboration mechanism into the RAG pipeline. To
better understand the long-term and session behavior of the user, ARAG
leverages four specialized LLM-based agents: a User Understanding Agent that
summarizes user preferences from long-term and session contexts, a Natural
Language Inference (NLI) Agent that evaluates semantic alignment between
candidate items retrieved by RAG and inferred intent, a context summary agent
that summarizes the findings of NLI agent, and an Item Ranker Agent that
generates a ranked list of recommendations based on contextual fit. We evaluate
ARAG accross three datasets. Experimental results demonstrate that ARAG
significantly outperforms standard RAG and recency-based baselines, achieving
up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an
ablation study to analyse the effect by different components of ARAG. Our
findings highlight the effectiveness of integrating agentic reasoning into
retrieval-augmented recommendation and provide new directions for LLM-based
personalization.

</details>


### [13] [CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design](https://arxiv.org/abs/2506.21934)
*Najmeh Forouzandehmehr,Reza Yousefi Maragheh,Sriram Kollipara,Kai Zhao,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: 本文介绍了CAL-RAG，一个具有检索增强和智能代理功能的框架，用于内容感知布局自动生成。


<details>
  <summary>Details</summary>
Motivation: 在智能设计系统中，自动内容感知布局生成这一任务虽然重要但研究不足。现有方法缺乏基于实例的上下文设计和处理语义对齐与视觉一致性的能力。

Method: 本文提出的CAL-RAG框架整合了多模态检索、大型语言模型和协作代理推理。从结构化的知识库中检索相关布局示例，并调用基于LLM的布局推荐器进行元素位置建议。使用视觉语言评分代理评估布局并使用反馈代理提供有针对性的改进。

Result: CAL-RAG在PKU海报布局数据集上实现了多项布局指标的最佳性能，超越了许多强基线，比如LayoutPrompter。

Conclusion: 本文结果表明，将检索增强与智能多步推理结合，能够为自动布局生成提供一个可扩展、可解释且高保真的解决方案。

Abstract: Automated content-aware layout generation -- the task of arranging visual
elements such as text, logos, and underlays on a background canvas -- remains a
fundamental yet under-explored problem in intelligent design systems. While
recent advances in deep generative models and large language models (LLMs) have
shown promise in structured content generation, most existing approaches lack
grounding in contextual design exemplars and fall short in handling semantic
alignment and visual coherence. In this work we introduce CAL-RAG, a
retrieval-augmented, agentic framework for content-aware layout generation that
integrates multimodal retrieval, large language models, and collaborative
agentic reasoning. Our system retrieves relevant layout examples from a
structured knowledge base and invokes an LLM-based layout recommender to
propose structured element placements. A vision-language grader agent evaluates
the layout with visual metrics, and a feedback agent provides targeted
refinements, enabling iterative improvement. We implement our framework using
LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in
semantic and structural variability. CAL-RAG achieves state-of-the-art
performance across multiple layout metrics -- including underlay effectiveness,
element alignment, and overlap -- substantially outperforming strong baselines
such as LayoutPrompter. These results demonstrate that combining retrieval
augmentation with agentic multi-step reasoning yields a scalable,
interpretable, and high-fidelity solution for automated layout generation.

</details>


### [14] [Literature-Grounded Novelty Assessment of Scientific Ideas](https://arxiv.org/abs/2506.22026)
*Simra Shahid,Marissa Radensky,Raymond Fok,Pao Siangliulue,Daniel S. Weld,Tom Hope*

Main category: cs.IR

TL;DR: 提出Idea Novelty Checker系统，基于LLM检索增强生成来解决科学想法自动新颖性评估的难题，并取得比现有方法高约13%的 专家一致性。


<details>
  <summary>Details</summary>
Motivation: 自动化科学想法生成系统虽有一定进步，但想法新鮮性自动评估仍是一个关键但尚未充分探索的挑战.

Method: 提出了基于LLM检索增强生成（RAG）的Idea Novelty Checker框架，使用两阶段检索然后重新排序方法（关键词和片段检索，基于嵌入的过滤和基于类别的LLM重新排序），并加入专家标注的例子指导新鮮性评估和生成基于文献的推理。

Result: 实验结果表明，该新颖性检查器比现有方法实现了高达13%的专家一致性。

Conclusion: 该方法有效地解决了科学想法自动新鮮性评估的问题，表明基于LLM的检索增强生成（RAG）方法在这一领域很有潜力。

Abstract: Automated scientific idea generation systems have made remarkable progress,
yet the automatic evaluation of idea novelty remains a critical and
underexplored challenge. Manual evaluation of novelty through literature review
is labor-intensive, prone to error due to subjectivity, and impractical at
scale. To address these issues, we propose the Idea Novelty Checker, an
LLM-based retrieval-augmented generation (RAG) framework that leverages a
two-stage retrieve-then-rerank approach. The Idea Novelty Checker first
collects a broad set of relevant papers using keyword and snippet-based
retrieval, then refines this collection through embedding-based filtering
followed by facet-based LLM re-ranking. It incorporates expert-labeled examples
to guide the system in comparing papers for novelty evaluation and in
generating literature-grounded reasoning. Our extensive experiments demonstrate
that our novelty checker achieves approximately 13% higher agreement than
existing approaches. Ablation studies further showcases the importance of the
facet-based re-ranker in identifying the most relevant literature for novelty
evaluation.

</details>


### [15] [Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems](https://arxiv.org/abs/2506.22112)
*Wenzheng Shu,Yanxiang Zeng,Yongxiang Tang,Teng Sha,Ning Luo,Yanhua Cheng,Xialong Liu,Fan Zhou,Peng Jiang*

Main category: cs.IR

TL;DR: 该论文提出了一个名为R3S的离线强化学习框架，用于改进推荐系统，通过整合模型不确定性处理奖励预测中的内部波动，提高决策的多样性，并引入衰减惩罚器来阻止导致状态多样性减少的动作。实验结果表明R3S提高了世界模型的准确性，并有效地调和了用户的异构偏好。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习已成为现实世界推荐系统的流行且有效的方法，但奖励塑形仍面临挑战，以往的努力集中在改进世界模型或惩罚未被充分探索的状态-动作对。尽管如此，一个关键问题仍然存在：同时平衡世界模型中的内在偏差和策略推荐的多样性。

Method: 本文提出了名为'再分配奖励推荐系统'（R3S）的离线强化学习框架，通过整合模型不确定性，处理奖励预测中的内在波动，提高决策的多样性，并引入带衰减的惩罚器阻止导致状态多样性减少的动作。

Result: 实验结果表明，R3S提高了世界模型的准确性，有效地调和了用户的异构偏好，并通过引入额外的惩罚器成功地增加了决策的多样性。

Conclusion: 本论文提出的R3S框架有效地解决了离线强化学习在推荐系统中的关键问题，平衡了世界模型的内在偏差和策略推荐的多样性，展示了其在实际应用中的潜力。

Abstract: Offline reinforcement learning (RL) has emerged as a prevalent and effective
methodology for real-world recommender systems, enabling learning policies from
historical data and capturing user preferences. In offline RL, reward shaping
encounters significant challenges, with past efforts to incorporate prior
strategies for uncertainty to improve world models or penalize underexplored
state-action pairs. Despite these efforts, a critical gap remains: the
simultaneous balancing of intrinsic biases in world models and the diversity of
policy recommendations. To address this limitation, we present an innovative
offline RL framework termed Reallocated Reward for Recommender Systems (R3S).
By integrating inherent model uncertainty to tackle the intrinsic fluctuations
in reward predictions, we boost diversity for decision-making to align with a
more interactive paradigm, incorporating extra penalizers with decay that deter
actions leading to diminished state variety at both local and global scales.
The experimental results demonstrate that R3S improves the accuracy of world
models and efficiently harmonizes the heterogeneous preferences of the users.

</details>


### [16] [UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses](https://arxiv.org/abs/2506.22210)
*Weronika Łajewska,Ivica Kostric,Gabriel Iturra-Bocaz,Mariam Arustashvili,Krisztian Balog*

Main category: cs.IR

TL;DR: LiveRAG挑战旨在通过使用固定语料库和共享的大语言模型提升检索增强生成(RAG)的研究，克服RAG在事实准确性、来源归属及响应完整性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于检索增强生成(RAG)面上的事实准确性、来源归属及响应完整性方面的挑战，我们提出了一个提取最小、原子单位信息的方法，目标是改进RAG的研究。

Method: 我们提出了一个模块化管道处理信息零碎，这个多阶段管道包括查询重写、段落检索与重新排序、零碎检测与聚类、集群排序与总括、响应流畅性增强。

Result: 我们的结果显示结合原始查询与一些子查询重写可以提高召回率，而增加用于重新排序和生成文档的数量超过某个点后会减少效率且不能提高响应质量。

Conclusion: 研究发现结合原始查询与子查询重写能够提升RAG的有效性，同时在处理时要注意文档的数量控制，以达到效率和质量的平衡。

Abstract: Retrieval-augmented generation (RAG) faces challenges related to factual
correctness, source attribution, and response completeness. The LiveRAG
Challenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus
and a shared, open-source LLM. We propose a modular pipeline that operates on
information nuggets-minimal, atomic units of relevant information extracted
from retrieved documents. This multistage pipeline encompasses query rewriting,
passage retrieval and reranking, nugget detection and clustering, cluster
ranking and summarization, and response fluency enhancement. This design
inherently promotes grounding in specific facts, facilitates source
attribution, and ensures maximum information inclusion within length
constraints. In this challenge, we extend our focus to also address the
retrieval component of RAG, building upon our prior work on multi-faceted query
rewriting. Furthermore, for augmented generation, we concentrate on improving
context curation capabilities, maximizing the breadth of information covered in
the response while ensuring pipeline efficiency. Our results show that
combining original queries with a few sub-query rewrites boosts recall, while
increasing the number of documents used for reranking and generation beyond a
certain point reduces effectiveness, without improving response quality.

</details>


### [17] [JointRank: Rank Large Set with Single Pass](https://arxiv.org/abs/2506.22262)
*Evgeny Dedov*

Main category: cs.IR

TL;DR: 该方法通过分块并行独立排名，并使用隐式成对比较和全局排名算法来优化大型数据集的重排序。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有模型在处理大数据集时输入大小限制和排名质量下降的问题，提出了这个增强重排序的方法。

Method: 将候选项分块，每个块独立并行排名，然后从局部排名中推导出隐式成对比较，并使用Winrate或PageRank算法聚合比较以构建全局排名。

Result: 在TREC DL-2019实验中，与使用gpt-4.1-mini作为长文本模型的完整上下文列表方法相比，该方法实现了nDCG@10 70.88的排名，同时将延迟从21秒减少到8秒。

Conclusion: 该方法有效地提高了处理大型数据集时的排名效率和准确性，并能显著减少处理时间。

Abstract: Efficiently ranking relevant items from large candidate pools is a
cornerstone of modern information retrieval systems -- such as web search,
recommendation, and retrieval-augmented generation. Listwise rerankers, which
improve relevance by jointly considering multiple candidates, are often limited
in practice: either by model input size constraints, or by degraded quality
when processing large sets. We propose a model-agnostic method for fast
reranking large sets that exceed a model input limits. The method first
partitions candidate items into overlapping blocks, each of which is ranked
independently in parallel. Implicit pairwise comparisons are then derived from
these local rankings. Finally, these comparisons are aggregated to construct a
global ranking using algorithms such as Winrate or PageRank. Experiments on
TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the
57.68 for full-context listwise approach using gpt-4.1-mini as long-context
model, while reducing latency from 21 to 8 seconds.
  The implementation of the algorithm and the experiments is available in the
repository: https://github.com/V3RGANz/jointrank

</details>


### [18] [Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation](https://arxiv.org/abs/2506.22303)
*Xinghe Cheng,Zihan Zhang,Jiapu Wang,Liangda Fang,Chaobo He,Quanlong Guan,Shirui Pan,Weiqi Luo*

Main category: cs.IR

TL;DR: 本文提出了一种名为DLELP的新方法，通过整合先修和相似关系来增强学习路径推荐，以解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有学习路径推荐主要依赖先修关系，但教育数据集中的先修关系往往不明确，仅依赖先修关系会影响学习进程和结果。

Method: 文章 introducing a knowledge concept structure graph generation module生成知识概念结构图，并结合Discrimination Learning-driven Reinforcement Learning (DLRL)框架来改进推荐效果。

Result: 实验证明，该方法在三个基准数据集上达到了最优性能，并且能提供可解释的学习路径推荐理由。

Conclusion: DLELP方法有效地提高了学习路径推荐的可靠性和解释性，为学习者提供了更优的学习体验。

Abstract: Learning path recommendation seeks to provide learners with a structured
sequence of learning items (e.g., knowledge concepts or exercises) to optimize
their learning efficiency. Despite significant efforts in this area, most
existing methods primarily rely on prerequisite relationships, which present
two major limitations: 1) Many educational datasets do not explicitly provide
prerequisite relationships between knowledge concepts, hindering the
application of current learning path recommendation methods. 2) Relying solely
on prerequisite relationships as the sole knowledge structure can impede
learning progress and negatively impact student outcomes. To address these
challenges, we propose a novel approach, Discrimination Learning Enhances
Learning Path Recommendation (DLELP), which enhances learning path
recommendations by incorporating both prerequisite and similarity relationships
between knowledge concepts. Specifically, we introduce a knowledge concept
structure graph generation module that adaptively constructs knowledge concept
structure graphs for different educational datasets, significantly improving
the generalizability of learning path recommendation methods. We then propose a
Discrimination Learning-driven Reinforcement Learning (DLRL) framework, which
mitigates the issue of blocked learning paths, further enhancing the efficacy
of learning path recommendations. Finally, we conduct extensive experiments on
three benchmark datasets, demonstrating that our method not only achieves
state-of-the-art performance but also provides interpretable reasoning for the
recommended learning paths.

</details>


### [19] [HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval](https://arxiv.org/abs/2506.22356)
*Kevin Duh,Eugene Yang,Orion Weller,Andrew Yates,Dawn Lawrie*

Main category: cs.IR

TL;DR: 使用GPT-researcher框架和ColBERT bi-encoder架构，集成PLAID-X压缩索引、m2-bert-80M-8k-retrieval过滤器，最终使用Falcon3-10B生成答案，该系统在LiveRAG评估中得分为1.07，排名第五。


<details>
  <summary>Details</summary>
Motivation: 论文旨在通过优化研究问题的上下文、过滤结果以及生成最终答案的过程，提高信息检索的准确性和效率。

Method: 采用GPT-researcher框架进行研究与上下文分析，结合ColBERT bi-encoder架构进行检索，使用PLAID-X创建压缩索引，m2-bert-80M-8k-retrieval进行结果过滤，最终用Falcon3-10B生成答案。

Result: 该系统在LiveRAG自动评估中正确性排名第五，得分为1.07。

Conclusion: 通过综合使用GPT-researcher、ColBERT和多种模型技术，有效提高了检索系统的性能和评估结果。

Abstract: The HLTCOE LiveRAG submission utilized the GPT-researcher framework for
researching the context of the question, filtering the returned results, and
generating the final answer. The retrieval system was a ColBERT bi-encoder
architecture, which represents a passage with many dense tokens. Retrieval used
a local, compressed index of the FineWeb10-BT collection created with PLAID-X,
using a model fine-tuned for multilingual retrieval. Query generation from
context was done with Qwen2.5-7B-Instruct, while filtering was accomplished
with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to
generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG
automatic evaluation for correctness with a score of 1.07.

</details>


### [20] [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
*Maryam Mousavian,Zahra Abbasiantaeb,Mohammad Aliannejadi,Fabio Crestani*

Main category: cs.IR

TL;DR: 该论文利用大型语言模型(LLM)检测和测量文章排名中的性别偏见，并提出了一个新的性别公平度指标CWEx来克服现有指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理和信息检索系统中的社会偏见是持续挑战，因此需要开发强大的方法来识别和评估这些偏见。

Method: 构建基于LLM的性别偏见检测方法，并提出新的性别公平度指标CWEx，旨在解决现有方法的局限。

Result: 实验结果显示CWEx指标相比于之前的指标能更详细地评估公平性，并且与人类标签的符合度更高，有效地区分了排名中的性别偏见。

Conclusion: 集成LLM驱动的偏见检测、改进的公平度指标以及性别偏见标注，为分析并减少信息检索系统中的偏见提供了一个更强大的框架。

Abstract: The presence of social biases in Natural Language Processing (NLP) and
Information Retrieval (IR) systems is an ongoing challenge, which underlines
the importance of developing robust approaches to identifying and evaluating
such biases. In this paper, we aim to address this issue by leveraging Large
Language Models (LLMs) to detect and measure gender bias in passage ranking.
Existing gender fairness metrics rely on lexical- and frequency-based measures,
leading to various limitations, e.g., missing subtle gender disparities.
Building on our LLM-based gender bias detection method, we introduce a novel
gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to
address existing limitations. To measure the effectiveness of our proposed
metric and study LLMs' effectiveness in detecting gender bias, we annotate a
subset of the MS MARCO Passage Ranking collection and release our new gender
bias collection, called MSMGenderBias, to foster future research in this area.
Our extensive experimental results on various ranking models show that our
proposed metric offers a more detailed evaluation of fairness compared to
previous metrics, with improved alignment to human labels (58.77% for
Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa
agreement), effectively distinguishing gender bias in ranking. By integrating
LLM-driven bias detection, an improved fairness metric, and gender bias
annotations for an established dataset, this work provides a more robust
framework for analyzing and mitigating bias in IR systems.

</details>
